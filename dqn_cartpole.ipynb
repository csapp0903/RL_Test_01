# ==========================================
# 1. 环境设置与依赖安装 (Google Colab 专用)
# ==========================================
import sys
import subprocess
import os

def install_packages():
    print("正在检查并安装依赖环境...")
    packages = ["gymnasium[classic_control]", "matplotlib", "torch", "numpy"]
    for package in packages:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    print("依赖安装完成！")

# 如果是在 Colab 或本地没有这些库，尝试安装
try:
    import gymnasium as gym
    import torch
except ImportError:
    install_packages()
    import gymnasium as gym
    import torch

import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import random
from collections import deque
import math

# 检测是否可以使用 GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用计算设备: {device}")

# ==========================================
# 2. 超参数设置 (你可以调整这里来改变训练效果)
# ==========================================
BATCH_SIZE = 128        # 每次从经验池取出的样本数
GAMMA = 0.99            # 折扣因子 (关注长远利益)
EPS_START = 0.9         # 初始探索率 (随机动作概率)
EPS_END = 0.05          # 最终探索率
EPS_DECAY = 1000        # 探索率衰减速度
TAU = 0.005             # 软更新系数 (Target Network更新速度)
LR = 1e-4               # 学习率
MEMORY_CAPACITY = 10000 # 经验回放池大小
EPISODES = 300          # 训练的总回合数 (CartPole通常200-300次即可收敛)

# ==========================================
# 3. 核心组件定义
# ==========================================

# --- Q-Network 神经网络 ---
# 输入是状态(State)，输出是每个动作(Action)的Q值
class DQN(nn.Module):
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

# --- 经验回放池 (Replay Memory) ---
# 用于存储 (状态, 动作, 下一状态, 奖励)
class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, state, action, next_state, reward, done):
        """保存一条经验"""
        self.memory.append((state, action, next_state, reward, done))

    def sample(self, batch_size):
        """随机采样一个 Batch"""
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

# ==========================================
# 4. 辅助函数
# ==========================================

steps_done = 0

def select_action(state, policy_net, env):
    """Epsilon-Greedy 策略选择动作"""
    global steps_done
    sample = random.random()
    # 计算当前的探索阈值
    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1

    # 探索 (Exploration): 随机选择动作
    if sample < eps_threshold:
        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)
    # 利用 (Exploitation): 选择模型认为最好的动作
    else:
        with torch.no_grad():
            return policy_net(state).max(1)[1].view(1, 1)

def optimize_model(memory, policy_net, target_net, optimizer):
    """训练一步网络"""
    if len(memory) < BATCH_SIZE:
        return None

    # 1. 从经验池采样
    transitions = memory.sample(BATCH_SIZE)
    # 解压缩数据: batch_state, batch_action, ...
    batch_state, batch_action, batch_next_state, batch_reward, batch_done = zip(*transitions)

    state_batch = torch.cat(batch_state)
    action_batch = torch.cat(batch_action)
    reward_batch = torch.cat(batch_reward)
    
    # 处理非最终状态 (Done=False)
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch_next_state if s is not None])

    # 2. 计算当前的 Q(s, a)
    # gather(1, action_batch) 意味着我们只取实际上执行的那个动作对应的Q值
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # 3. 计算目标 Q 值: R + gamma * max(Q(s', a'))
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
    
    # 如果是终止状态，done为True (在公式里我们用1-done处理，这里简化逻辑直接用mask)
    # 注意：CartPole如果失败done=True，目标值就只是Reward
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # 4. 计算 Loss (Huber Loss 比 MSE 更稳定)
    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

    # 5. 反向传播与优化
    optimizer.zero_grad()
    loss.backward()
    # 梯度裁剪，防止梯度爆炸
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    optimizer.step()

    return loss.item()

# ==========================================
# 5. 主训练循环
# ==========================================

def train_dqn():
    print("开始初始化环境 CartPole-v1 ...")
    env = gym.make("CartPole-v1")
    
    # 获取状态和动作的数量
    n_actions = env.action_space.n
    state, _ = env.reset()
    n_observations = len(state)

    # 初始化网络
    policy_net = DQN(n_observations, n_actions).to(device)
    target_net = DQN(n_observations, n_actions).to(device)
    target_net.load_state_dict(policy_net.state_dict()) # 同步参数

    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
    memory = ReplayMemory(MEMORY_CAPACITY)

    # 用于记录绘图数据
    episode_rewards = []
    episode_losses = []
    
    print(f"开始训练，计划运行 {EPISODES} 个回合...")
    print("-" * 50)

    for i_episode in range(EPISODES):
        state, info = env.reset()
        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
        
        total_reward = 0
        losses = []

        for t in range(1000): # 每个回合最大步数
            # 1. 选择动作
            action = select_action(state, policy_net, env)
            
            # 2. 执行动作
            observation, reward, terminated, truncated, _ = env.step(action.item())
            done = terminated or truncated
            
            if terminated:
                next_state = None
            else:
                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)
            
            reward_tensor = torch.tensor([reward], device=device)
            total_reward += reward

            # 3. 存入经验池
            memory.push(state, action, next_state, reward_tensor, done)
            state = next_state

            # 4. 优化模型
            loss_val = optimize_model(memory, policy_net, target_net, optimizer)
            if loss_val is not None:
                losses.append(loss_val)

            # 5. 软更新目标网络 (Soft Update)
            # 这一步让 Target Net 缓慢追随 Policy Net，增加训练稳定性
            target_net_state_dict = target_net.state_dict()
            policy_net_state_dict = policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
            target_net.load_state_dict(target_net_state_dict)

            if done:
                break
        
        # 记录本回合数据
        episode_rewards.append(total_reward)
        avg_loss = np.mean(losses) if losses else 0
        episode_losses.append(avg_loss)

        # 打印进度 (每10回合)
        if (i_episode + 1) % 10 == 0:
            print(f"回合: {i_episode + 1}/{EPISODES} | "
                  f"本回合奖励: {total_reward:.1f} | "
                  f"平均 Loss: {avg_loss:.4f} | "
                  f"Epsilon: {EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY):.2f}")

    print("-" * 50)
    print("训练结束！")
    
    # 保存模型
    torch.save(policy_net.state_dict(), "cartpole_dqn_model.pth")
    print("模型已保存为 cartpole_dqn_model.pth")
    
    return episode_rewards, episode_losses

# ==========================================
# 6. 结果可视化 (详细)
# ==========================================
def plot_results(rewards, losses):
    plt.figure(figsize=(15, 5))

    # 子图1: 奖励曲线
    plt.subplot(1, 2, 1)
    plt.plot(rewards, label='Reward', color='blue', alpha=0.6)
    # 计算移动平均线 (让趋势更明显)
    window_size = 20
    if len(rewards) >= window_size:
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        plt.plot(range(window_size-1, len(rewards)), moving_avg, label='20-Episode Moving Avg', color='red', linewidth=2)
    
    plt.title('Training Rewards (Higher is Better)')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 子图2: Loss 曲线
    plt.subplot(1, 2, 2)
    plt.plot(losses, label='Loss', color='orange', alpha=0.6)
    plt.title('Training Loss (Lower is Better)')
    plt.xlabel('Episode')
    plt.ylabel('Average Loss')
    plt.legend()
    plt.yscale('log') # Loss通常变化范围大，用对数坐标更清晰
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ==========================================
# 7. 执行入口
# ==========================================
if __name__ == "__main__":
    # 运行训练
    rewards, losses = train_dqn()
    
    # 绘制图表
    plot_results(rewards, losses)

    print("\n解读提示：")
    print("1. 左图(Rewards)：如果红线(移动平均)逐渐上升并稳定在最高点(CartPole-v1通常是500或在此代码限制下较短)，说明智能体学会了游戏。")
    print("2. 右图(Loss)：Loss可能会震荡，这是DQN的正常现象，只要没有持续指数级爆炸即可。")
