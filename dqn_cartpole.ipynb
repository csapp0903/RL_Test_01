{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Test 01: DQN CartPole (PyTorch)\n",
    "\n",
    "这是一个使用 Deep Q-Network (DQN) 解决 CartPole-v1 问题的完整示例。\n",
    "本 Notebook 包含环境安装、模型定义、训练循环以及详细的结果可视化。\n",
    "\n",
    "**可以直接在 Google Colab 中点击运行。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 安装依赖 (适配 Google Colab 环境)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"正在检查并安装依赖环境...\")\n",
    "!pip install gymnasium[classic_control] matplotlib torch numpy\n",
    "print(\"依赖安装完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 导入库与环境设置\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置 matplotlib 在 notebook 中直接显示\n",
    "%matplotlib inline\n",
    "\n",
    "# 检测 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用计算设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 超参数配置\n",
    "BATCH_SIZE = 128        # 批量大小\n",
    "GAMMA = 0.99            # 折扣因子\n",
    "EPS_START = 0.9         # 初始探索率\n",
    "EPS_END = 0.05          # 最小探索率\n",
    "EPS_DECAY = 1000        # 探索率衰减速率\n",
    "TAU = 0.005             # 目标网络软更新系数\n",
    "LR = 1e-4               # 学习率\n",
    "MEMORY_CAPACITY = 10000 # 经验回放池容量\n",
    "EPISODES = 300          # 训练总回合数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 定义核心类：经验回放池与神经网络\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        \"\"\"保存一条经验\"\"\"\n",
    "        self.memory.append((state, action, next_state, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"随机采样\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 辅助函数：动作选择与模型优化\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state, policy_net, env):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample < eps_threshold:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "def optimize_model(memory, policy_net, target_net, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return None\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward, batch_done = zip(*transitions)\n",
    "\n",
    "    state_batch = torch.cat(batch_state)\n",
    "    action_batch = torch.cat(batch_action)\n",
    "    reward_batch = torch.cat(batch_reward)\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch_next_state if s is not None])\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 训练主循环\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_actions = env.action_space.n\n",
    "state, _ = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "\n",
    "print(f\"开始训练 {EPISODES} 个回合...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i_episode in range(EPISODES):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    total_reward = 0\n",
    "    losses = []\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state, policy_net, env)\n",
