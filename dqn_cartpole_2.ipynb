{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csapp0903/RL_Test_01/blob/main/dqn_cartpole_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meer8ZHFHxdv"
      },
      "source": [
        "# RL Test 01: DQN CartPole (PyTorch)\n",
        "\n",
        "这是一个使用 Deep Q-Network (DQN) 解决 CartPole-v1 问题的完整示例。\n",
        "本 Notebook 包含环境安装、模型定义、训练循环以及详细的结果可视化。\n",
        "\n",
        "**可以直接在 Google Colab 中点击运行。**"
      ],
      "id": "meer8ZHFHxdv"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2f8cQD8IHxdw",
        "outputId": "c1c36f76-6194-4a2b-8526-8b3b30cb95c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在检查并安装依赖环境...\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "依赖安装完成！\n"
          ]
        }
      ],
      "source": [
        "# 1. 安装依赖 (适配 Google Colab 环境)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"正在检查并安装依赖环境...\")\n",
        "!pip install gymnasium[classic_control] matplotlib torch numpy\n",
        "print(\"依赖安装完成！\")"
      ],
      "id": "2f8cQD8IHxdw"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ousowSBIHxdx",
        "outputId": "eec1f70f-4fea-40fd-b02b-b5cf70bbd9f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用计算设备: cpu\n"
          ]
        }
      ],
      "source": [
        "# 2. 导入库与环境设置\n",
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 设置 matplotlib 在 notebook 中直接显示\n",
        "%matplotlib inline\n",
        "\n",
        "# 检测 GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用计算设备: {device}\")"
      ],
      "id": "ousowSBIHxdx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fdgfcoeHxdx"
      },
      "outputs": [],
      "source": [
        "# 3. 超参数配置\n",
        "BATCH_SIZE = 128        # 批量大小\n",
        "GAMMA = 0.99            # 折扣因子\n",
        "EPS_START = 0.9         # 初始探索率\n",
        "EPS_END = 0.05          # 最小探索率\n",
        "EPS_DECAY = 1000        # 探索率衰减速率\n",
        "TAU = 0.005             # 目标网络软更新系数\n",
        "LR = 1e-4               # 学习率\n",
        "MEMORY_CAPACITY = 10000 # 经验回放池容量\n",
        "EPISODES = 300          # 训练总回合数"
      ],
      "id": "_fdgfcoeHxdx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCCQu3SQHxdy"
      },
      "outputs": [],
      "source": [
        "# 4. 定义核心类：经验回放池与神经网络\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "        \"\"\"保存一条经验\"\"\"\n",
        "        self.memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"随机采样\"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ],
      "id": "pCCQu3SQHxdy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsCJYn94Hxdy"
      },
      "outputs": [],
      "source": [
        "# 5. 辅助函数：动作选择与模型优化\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state, policy_net, env):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "\n",
        "    if sample < eps_threshold:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "\n",
        "def optimize_model(memory, policy_net, target_net, optimizer):\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return None\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch_state, batch_action, batch_next_state, batch_reward, batch_done = zip(*transitions)\n",
        "\n",
        "    state_batch = torch.cat(batch_state)\n",
        "    action_batch = torch.cat(batch_action)\n",
        "    reward_batch = torch.cat(batch_reward)\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch_next_state if s is not None])\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "id": "lsCJYn94Hxdy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiR_yLQLHxdy"
      },
      "outputs": [],
      "source": [
        "# 6. 训练主循环\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(MEMORY_CAPACITY)\n",
        "\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "\n",
        "print(f\"开始训练 {EPISODES} 个回合...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i_episode in range(EPISODES):\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    total_reward = 0\n",
        "    losses = []\n",
        "\n",
        "    for t in count():\n",
        "        action = select_action(state, policy_net, env)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        reward_tensor = torch.tensor([reward], device=device)\n",
        "        total_reward += reward\n",
        "        memory.push(state, action, next_state, reward_tensor, done)\n",
        "        state = next_state\n",
        "\n",
        "        loss_val = optimize_model(memory, policy_net, target_net, optimizer)\n",
        "        if loss_val is not None:\n",
        "            losses.append(loss_val)\n",
        "\n",
        "        # 软更新目标网络\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_losses.append(np.mean(losses) if losses else 0)\n",
        "\n",
        "    if (i_episode + 1) % 20 == 0:\n",
        "        print(f\"Episode {i_episode + 1}/{EPISODES} | Reward: {total_reward:.1f} | Avg Loss: {episode_losses[-1]:.4f}\")\n",
        "\n",
        "print(\"训练结束！\")\n",
        "torch.save(policy_net.state_dict(), \"cartpole_dqn.pth\")"
      ],
      "id": "QiR_yLQLHxdy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6mnIEeJHxdz"
      },
      "outputs": [],
      "source": [
        "# 7. 结果可视化\n",
        "def plot_results(rewards, losses):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards, label='Reward', color='blue', alpha=0.6)\n",
        "    # 移动平均线\n",
        "    window = 20\n",
        "    if len(rewards) >= window:\n",
        "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "        plt.plot(range(window-1, len(rewards)), moving_avg, label='20-Ep Moving Avg', color='red')\n",
        "    plt.title('Training Rewards')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(losses, label='Loss', color='orange', alpha=0.6)\n",
        "    plt.title('Training Loss (Log Scale)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(episode_rewards, episode_losses)"
      ],
      "id": "i6mnIEeJHxdz"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}